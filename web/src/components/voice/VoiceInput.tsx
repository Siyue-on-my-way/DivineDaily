import { useState, useEffect, useRef } from 'react';
import { motion } from 'framer-motion';
import './VoiceInput.css';

interface Props {
  onTranscript: (text: string) => void;
  disabled?: boolean;
}

export default function VoiceInput({ onTranscript, disabled = false }: Props) {
  const [isRecording, setIsRecording] = useState(false);
  const [transcript, setTranscript] = useState('');
  const [error, setError] = useState('');
  const [hasPermission, setHasPermission] = useState<boolean | null>(null);
  const [audioLevel, setAudioLevel] = useState(0);
  
  const recognitionRef = useRef<any>(null);
  const audioContextRef = useRef<AudioContext | null>(null);
  const analyserRef = useRef<AnalyserNode | null>(null);
  const animationFrameRef = useRef<number | null>(null);

  useEffect(() => {
    // æ£€æŸ¥æµè§ˆå™¨æ”¯æŒ
    const SpeechRecognition = (window as any).SpeechRecognition || (window as any).webkitSpeechRecognition;
    
    if (!SpeechRecognition) {
      setError('æ‚¨çš„æµè§ˆå™¨ä¸æ”¯æŒè¯­éŸ³è¯†åˆ«åŠŸèƒ½');
      return;
    }

    // åˆå§‹åŒ–è¯­éŸ³è¯†åˆ«
    const recognition = new SpeechRecognition();
    recognition.lang = 'zh-CN';
    recognition.continuous = false;
    recognition.interimResults = true;

    recognition.onresult = (event: any) => {
      const current = event.resultIndex;
      const transcriptText = event.results[current][0].transcript;
      setTranscript(transcriptText);

      // å¦‚æœæ˜¯æœ€ç»ˆç»“æœ
      if (event.results[current].isFinal) {
        onTranscript(transcriptText);
        setIsRecording(false);
        stopAudioAnalysis();
      }
    };

    recognition.onerror = (event: any) => {
      console.error('Speech recognition error:', event.error);
      
      if (event.error === 'not-allowed') {
        setError('è¯·å…è®¸ä½¿ç”¨éº¦å…‹é£æƒé™');
        setHasPermission(false);
      } else if (event.error === 'no-speech') {
        setError('æœªæ£€æµ‹åˆ°è¯­éŸ³ï¼Œè¯·é‡è¯•');
      } else {
        setError('è¯­éŸ³è¯†åˆ«å¤±è´¥ï¼Œè¯·é‡è¯•');
      }
      
      setIsRecording(false);
      stopAudioAnalysis();
    };

    recognition.onend = () => {
      setIsRecording(false);
      stopAudioAnalysis();
    };

    recognitionRef.current = recognition;

    return () => {
      if (recognitionRef.current) {
        recognitionRef.current.stop();
      }
      stopAudioAnalysis();
    };
  }, [onTranscript]);

  const startRecording = async () => {
    if (disabled) return;

    setError('');
    setTranscript('');

    try {
      // è¯·æ±‚éº¦å…‹é£æƒé™
      const stream = await navigator.mediaDevices.getUserMedia({ audio: true });
      setHasPermission(true);

      // å¯åŠ¨éŸ³é¢‘åˆ†æ
      startAudioAnalysis(stream);

      // å¯åŠ¨è¯­éŸ³è¯†åˆ«
      if (recognitionRef.current) {
        recognitionRef.current.start();
        setIsRecording(true);
      }
    } catch (err) {
      console.error('Failed to start recording:', err);
      setError('æ— æ³•è®¿é—®éº¦å…‹é£ï¼Œè¯·æ£€æŸ¥æƒé™è®¾ç½®');
      setHasPermission(false);
    }
  };

  const stopRecording = () => {
    if (recognitionRef.current && isRecording) {
      recognitionRef.current.stop();
    }
    setIsRecording(false);
    stopAudioAnalysis();
  };

  const startAudioAnalysis = (stream: MediaStream) => {
    try {
      audioContextRef.current = new AudioContext();
      analyserRef.current = audioContextRef.current.createAnalyser();
      const source = audioContextRef.current.createMediaStreamSource(stream);
      source.connect(analyserRef.current);
      analyserRef.current.fftSize = 256;

      const dataArray = new Uint8Array(analyserRef.current.frequencyBinCount);

      const updateAudioLevel = () => {
        if (analyserRef.current) {
          analyserRef.current.getByteFrequencyData(dataArray);
          const average = dataArray.reduce((a, b) => a + b) / dataArray.length;
          setAudioLevel(average);
          animationFrameRef.current = requestAnimationFrame(updateAudioLevel);
        }
      };

      updateAudioLevel();
    } catch (err) {
      console.error('Failed to start audio analysis:', err);
    }
  };

  const stopAudioAnalysis = () => {
    if (animationFrameRef.current) {
      cancelAnimationFrame(animationFrameRef.current);
      animationFrameRef.current = null;
    }
    if (audioContextRef.current) {
      audioContextRef.current.close();
      audioContextRef.current = null;
    }
    setAudioLevel(0);
  };

  const requestPermission = async () => {
    try {
      await navigator.mediaDevices.getUserMedia({ audio: true });
      setHasPermission(true);
      setError('');
    } catch (err) {
      setError('æ— æ³•è·å–éº¦å…‹é£æƒé™');
      setHasPermission(false);
    }
  };

  if (hasPermission === false) {
    return (
      <div className="voice-input">
        <div className="voice-input__permission">
          <div className="voice-input__permission-icon">ğŸ¤</div>
          <div className="voice-input__permission-text">
            éœ€è¦éº¦å…‹é£æƒé™æ‰èƒ½ä½¿ç”¨è¯­éŸ³è¾“å…¥åŠŸèƒ½
          </div>
          <button
            className="voice-input__permission-button"
            onClick={requestPermission}
          >
            æˆäºˆæƒé™
          </button>
        </div>
      </div>
    );
  }

  return (
    <div className="voice-input">
      {/* å½•éŸ³æŒ‰é’® */}
      <motion.button
        className={`voice-input__button ${
          isRecording ? 'voice-input__button--recording' : ''
        } ${disabled ? 'voice-input__button--disabled' : ''}`}
        onMouseDown={startRecording}
        onMouseUp={stopRecording}
        onMouseLeave={stopRecording}
        onTouchStart={startRecording}
        onTouchEnd={stopRecording}
        disabled={disabled}
        whileHover={!disabled ? { scale: 1.05 } : {}}
        whileTap={!disabled ? { scale: 0.95 } : {}}
      >
        {isRecording ? 'ğŸ”´' : 'ğŸ¤'}
      </motion.button>

      {/* æç¤ºæ–‡å­— */}
      <div
        className={`voice-input__hint ${
          isRecording ? 'voice-input__hint--recording' : ''
        }`}
      >
        {isRecording ? 'æ¾å¼€å‘é€' : 'æŒ‰ä½è¯´è¯'}
      </div>

      {/* æ³¢å½¢æ˜¾ç¤º */}
      {isRecording && (
        <motion.div
          className="voice-input__waveform"
          initial={{ opacity: 0 }}
          animate={{ opacity: 1 }}
          exit={{ opacity: 0 }}
        >
          {Array.from({ length: 10 }).map((_, i) => (
            <div
              key={i}
              className="voice-input__wave-bar voice-input__wave-bar--recording"
              style={{
                height: `${Math.max(10, (audioLevel / 255) * 40 + Math.random() * 20)}px`,
                animationDelay: `${i * 0.1}s`
              }}
            />
          ))}
        </motion.div>
      )}

      {/* è¯†åˆ«ç»“æœ */}
      {transcript && (
        <motion.div
          className="voice-input__result"
          initial={{ opacity: 0, y: 10 }}
          animate={{ opacity: 1, y: 0 }}
        >
          {transcript}
        </motion.div>
      )}

      {/* é”™è¯¯æç¤º */}
      {error && (
        <motion.div
          className="voice-input__error"
          initial={{ opacity: 0, y: 10 }}
          animate={{ opacity: 1, y: 0 }}
        >
          {error}
        </motion.div>
      )}
    </div>
  );
}
