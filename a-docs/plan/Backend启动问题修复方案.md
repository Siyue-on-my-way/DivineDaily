# Backendå¯åŠ¨é—®é¢˜ä¿®å¤æ–¹æ¡ˆ

**é—®é¢˜**: Backendå®¹å™¨å¯åŠ¨å¤±è´¥  
**é”™è¯¯**: `ImportError: cannot import name 'create_llm_service' from 'app.services.llm_service'`

---

## ğŸ” é—®é¢˜åˆ†æ

### é”™è¯¯å †æ ˆ
```python
File "/app/app/services/enhanced_divination_service.py", line 9, in <module>
    from app.services.llm_service import LLMService, create_llm_service
ImportError: cannot import name 'create_llm_service' from 'app.services.llm_service'
```

### æ ¹æœ¬åŸå› 
`enhanced_divination_service.py` å¯¼å…¥äº† `create_llm_service` å‡½æ•°ï¼Œä½† `llm_service.py` ä¸­åªå®šä¹‰äº†ï¼š
- `LLMService` (æŠ½è±¡åŸºç±»)
- `MockLLMService` (Mockå®ç°)
- `get_llm_service()` (è¿”å›Mockå®ä¾‹)

**ç¼ºå°‘**: `create_llm_service()` å‡½æ•°

---

## ğŸ”§ ä¿®å¤æ–¹æ¡ˆ

### æ–¹æ¡ˆ1: æ·»åŠ  create_llm_service å‡½æ•°ï¼ˆæ¨èï¼‰

åœ¨ `llm_service.py` ä¸­æ·»åŠ å®Œæ•´çš„LLMæœåŠ¡åˆ›å»ºå‡½æ•°ï¼š

```python
"""LLMæœåŠ¡å±‚"""

import httpx
import json
from abc import ABC, abstractmethod
from typing import Optional, Dict, Any, List


class LLMService(ABC):
    """LLMæœåŠ¡æŠ½è±¡åŸºç±»"""
    
    @abstractmethod
    async def generate(self, prompt: str) -> str:
        """ç”Ÿæˆæ–‡æœ¬"""
        pass
    
    async def generate_answer(self, prompt: str) -> str:
        """ç”Ÿæˆç­”æ¡ˆï¼ˆåˆ«åï¼‰"""
        return await self.generate(prompt)
    
    async def generate_detail(self, prompt: str) -> str:
        """ç”Ÿæˆè¯¦æƒ…ï¼ˆåˆ«åï¼‰"""
        return await self.generate(prompt)
    
    async def close(self):
        """å…³é—­è¿æ¥"""
        pass


class MockLLMService(LLMService):
    """Mock LLMæœåŠ¡ï¼ˆç”¨äºæµ‹è¯•å’Œé™çº§ï¼‰"""
    
    async def generate(self, prompt: str) -> str:
        return "è¿™æ˜¯ä¸€ä¸ªæµ‹è¯•ç­”æ¡ˆã€‚æ ¹æ®åˆ†æï¼Œå»ºè®®æ‚¨è°¨æ…è€ƒè™‘ã€‚"


class OpenAICompatibleLLMService(LLMService):
    """OpenAIå…¼å®¹çš„LLMæœåŠ¡"""
    
    def __init__(
        self,
        endpoint: str,
        api_key: str,
        model_name: str,
        temperature: float = 0.7,
        max_tokens: int = 2000,
        timeout: int = 30
    ):
        self.endpoint = endpoint
        self.api_key = api_key
        self.model_name = model_name
        self.temperature = temperature
        self.max_tokens = max_tokens
        self.timeout = timeout
        self.client = httpx.AsyncClient(timeout=timeout)
    
    async def generate(self, prompt: str) -> str:
        """è°ƒç”¨LLMç”Ÿæˆæ–‡æœ¬"""
        headers = {
            "Content-Type": "application/json",
            "Authorization": f"Bearer {self.api_key}"
        }
        
        payload = {
            "model": self.model_name,
            "messages": [{"role": "user", "content": prompt}],
            "temperature": self.temperature,
            "max_tokens": self.max_tokens
        }
        
        try:
            response = await self.client.post(
                self.endpoint,
                headers=headers,
                json=payload
            )
            response.raise_for_status()
            
            data = response.json()
            if "choices" in data and len(data["choices"]) > 0:
                return data["choices"][0]["message"]["content"]
            else:
                raise ValueError("Invalid response format")
        
        except Exception as e:
            print(f"LLMè°ƒç”¨å¤±è´¥: {e}")
            # é™çº§åˆ°MockæœåŠ¡
            return await MockLLMService().generate(prompt)
    
    async def close(self):
        """å…³é—­HTTPå®¢æˆ·ç«¯"""
        await self.client.aclose()


def create_llm_service(
    llm_config,
    temperature: Optional[float] = None,
    max_tokens: Optional[int] = None,
    timeout: Optional[int] = None
) -> LLMService:
    """
    æ ¹æ®é…ç½®åˆ›å»ºLLMæœåŠ¡å®ä¾‹
    
    Args:
        llm_config: LLMé…ç½®å¯¹è±¡
        temperature: æ¸©åº¦å‚æ•°ï¼ˆå¯é€‰ï¼Œè¦†ç›–é…ç½®ï¼‰
        max_tokens: æœ€å¤§tokenæ•°ï¼ˆå¯é€‰ï¼Œè¦†ç›–é…ç½®ï¼‰
        timeout: è¶…æ—¶æ—¶é—´ï¼ˆå¯é€‰ï¼Œè¦†ç›–é…ç½®ï¼‰
    
    Returns:
        LLMServiceå®ä¾‹
    """
    # å¦‚æœæ²¡æœ‰é…ç½®ï¼Œè¿”å›MockæœåŠ¡
    if not llm_config:
        return MockLLMService()
    
    # å¦‚æœé…ç½®æœªå¯ç”¨ï¼Œè¿”å›MockæœåŠ¡
    if not llm_config.is_enabled:
        return MockLLMService()
    
    # è·å–é…ç½®å‚æ•°
    endpoint = llm_config.endpoint
    api_key = llm_config.api_key
    model_name = llm_config.model_name
    
    # ä½¿ç”¨ä¼ å…¥çš„å‚æ•°æˆ–é…ç½®ä¸­çš„å‚æ•°
    if temperature is None:
        temperature = llm_config.extra_config.get("temperature", 0.7) if llm_config.extra_config else 0.7
    
    if max_tokens is None:
        max_tokens = llm_config.extra_config.get("max_tokens", 2000) if llm_config.extra_config else 2000
    
    if timeout is None:
        timeout = llm_config.extra_config.get("timeout", 30) if llm_config.extra_config else 30
    
    # æ ¹æ®provideråˆ›å»ºå¯¹åº”çš„æœåŠ¡
    provider = llm_config.provider.lower()
    
    if provider in ["openai", "deepseek", "doubao"] or llm_config.url_type == "openai_compatible":
        return OpenAICompatibleLLMService(
            endpoint=endpoint,
            api_key=api_key,
            model_name=model_name,
            temperature=temperature,
            max_tokens=max_tokens,
            timeout=timeout
        )
    else:
        # æœªçŸ¥providerï¼Œè¿”å›MockæœåŠ¡
        print(f"æœªçŸ¥çš„LLM provider: {provider}ï¼Œä½¿ç”¨MockæœåŠ¡")
        return MockLLMService()


def get_llm_service() -> LLMService:
    """è·å–é»˜è®¤LLMæœåŠ¡å®ä¾‹ï¼ˆMockï¼‰"""
    return MockLLMService()
```

---

### æ–¹æ¡ˆ2: ä¿®æ”¹ enhanced_divination_service.pyï¼ˆä¸æ¨èï¼‰

å¦‚æœä¸æƒ³ä¿®æ”¹ `llm_service.py`ï¼Œå¯ä»¥ä¿®æ”¹å¯¼å…¥è¯­å¥ï¼š

```python
# ä¿®æ”¹å‰
from app.services.llm_service import LLMService, create_llm_service

# ä¿®æ”¹å
from app.services.llm_service import LLMService, get_llm_service
```

**ç¼ºç‚¹**: è¿™æ ·ä¼šå¤±å»æ ¹æ®é…ç½®åˆ›å»ºä¸åŒLLMæœåŠ¡çš„èƒ½åŠ›ã€‚

---

## ğŸš€ æ‰§è¡Œæ­¥éª¤

### æ­¥éª¤1: å¤‡ä»½åŸæ–‡ä»¶
```bash
cd /mnt/DivineDaily/backend-python/app/services
cp llm_service.py llm_service.py.backup
```

### æ­¥éª¤2: æ›´æ–° llm_service.py
ä½¿ç”¨ä¸Šé¢æ–¹æ¡ˆ1çš„å®Œæ•´ä»£ç æ›¿æ¢ `llm_service.py` çš„å†…å®¹ã€‚

### æ­¥éª¤3: é‡å¯Backendå®¹å™¨
```bash
cd /mnt/DivineDaily/docker
docker-compose restart backend-python
```

### æ­¥éª¤4: éªŒè¯æœåŠ¡å¯åŠ¨
```bash
# ç­‰å¾…10ç§’è®©æœåŠ¡å¯åŠ¨
sleep 10

# æ£€æŸ¥å¥åº·çŠ¶æ€
curl http://localhost:48080/health

# æŸ¥çœ‹æ—¥å¿—
docker-compose logs --tail=50 backend-python
```

### æ­¥éª¤5: è¿è¡Œæµ‹è¯•
```bash
cd /mnt/DivineDaily/backend-python/tests
python run_all_tests.py
```

---

## âœ… éªŒè¯æ¸…å•

- [ ] `llm_service.py` å·²æ›´æ–°
- [ ] Backendå®¹å™¨é‡å¯æˆåŠŸ
- [ ] å¥åº·æ£€æŸ¥è¿”å›æ­£å¸¸
- [ ] æ—¥å¿—ä¸­æ— é”™è¯¯ä¿¡æ¯
- [ ] æµ‹è¯•å¯ä»¥æ­£å¸¸è¿è¡Œ

---

## ğŸ“ é¢„æœŸç»“æœ

### æˆåŠŸæ ‡å¿—
```bash
# å¥åº·æ£€æŸ¥
$ curl http://localhost:48080/health
{
  "status": "healthy",
  "service": "DivineDaily Backend",
  "version": "1.0.0"
}

# å®¹å™¨çŠ¶æ€
$ docker ps | grep backend-python
divine-daily-backend-python   Up 2 minutes   0.0.0.0:48080->8080/tcp
```

### æµ‹è¯•è¾“å‡º
```
================================================================================
  DivineDaily å®Œæ•´æµ‹è¯•å¥—ä»¶
  æµ‹è¯•æ—¶é—´: 2026-02-16 19:30:00
================================================================================

...

æ€»è®¡: 35/35 é€šè¿‡ (100%)

ğŸ‰ æ‰€æœ‰æµ‹è¯•å¥—ä»¶é€šè¿‡ï¼
```

---

## ğŸ” æ•…éšœæ’æŸ¥

### å¦‚æœå®¹å™¨ä»ç„¶å¤±è´¥

1. **æŸ¥çœ‹è¯¦ç»†æ—¥å¿—**
```bash
docker-compose logs backend-python | tail -100
```

2. **æ£€æŸ¥Pythonè¯­æ³•**
```bash
cd /mnt/DivineDaily/backend-python
python -m py_compile app/services/llm_service.py
```

3. **è¿›å…¥å®¹å™¨è°ƒè¯•**
```bash
docker exec -it divine-daily-backend-python bash
cd /app
python -c "from app.services.llm_service import create_llm_service; print('OK')"
```

---

**åˆ›å»ºæ—¶é—´**: 2026-02-16  
**ä¼˜å…ˆçº§**: ğŸ”´ æœ€é«˜  
**çŠ¶æ€**: å¾…æ‰§è¡Œ

